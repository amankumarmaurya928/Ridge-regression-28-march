{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9d57296-d68c-4a6d-b1ab-32aa5c1fb5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ridge regression is a method of estimating the coefficients of multiple-regression models in scenarios where the\\n   independent variables are highly correlated. It has been used in many fields including econometrics, chemistry, and\\n   engineering.\\n   Ridge regression is a term used to refer to a linear regression model whose coefficients are estimated not by ordinary \\n   least squares (OLS), but by an estimator, called ridge estimator, that, albeit biased, has lower variance than the OLS\\n   estimator.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1\n",
    "'''Ridge regression is a method of estimating the coefficients of multiple-regression models in scenarios where the\n",
    "   independent variables are highly correlated. It has been used in many fields including econometrics, chemistry, and\n",
    "   engineering.\n",
    "   Ridge regression is a term used to refer to a linear regression model whose coefficients are estimated not by ordinary \n",
    "   least squares (OLS), but by an estimator, called ridge estimator, that, albeit biased, has lower variance than the OLS\n",
    "   estimator.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89fcae1e-1dfe-417e-90b5-132cf250360f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The assumptions of ridge regression are the same as that of linear regression: linearity, constant variance, and\\n   independence. However, as ridge regression does not provide confidence limits, the distribution of errors to be normal need\\n   not be assumed.\\n   '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q2\n",
    "'''The assumptions of ridge regression are the same as that of linear regression: linearity, constant variance, and\n",
    "   independence. However, as ridge regression does not provide confidence limits, the distribution of errors to be normal need\n",
    "   not be assumed.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4e1d641-eabd-479c-9916-e94bfffde4bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ridge regression\\n   Selecting a good value for λ is critical. When λ=0, the penalty term has no effect, and ridge regression will produce the \\n   classical least square coefficients. However, as λ increases to infinite, the impact of the shrinkage penalty grows, and \\n   the ridge regression coefficients will get close zero.\\n   When choosing a lambda value, the goal is to strike the right balance between simplicity and training-data fit: If your \\n   lambda value is too high, your model will be simple, but you run the risk of underfitting your data. Your model won't learn \\n   enough about the training data to make useful predictions.\\n   \""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3\n",
    "'''Ridge regression\n",
    "   Selecting a good value for λ is critical. When λ=0, the penalty term has no effect, and ridge regression will produce the \n",
    "   classical least square coefficients. However, as λ increases to infinite, the impact of the shrinkage penalty grows, and \n",
    "   the ridge regression coefficients will get close zero.\n",
    "   When choosing a lambda value, the goal is to strike the right balance between simplicity and training-data fit: If your \n",
    "   lambda value is too high, your model will be simple, but you run the risk of underfitting your data. Your model won't learn \n",
    "   enough about the training data to make useful predictions.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23b8de20-2cc9-48e5-8f5a-05e2eb13bd1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yes,\\n   We can use ridge regression for feature selection while fitting the model. In this article, we are going to use logistic \\n   regression for model fitting and push the parameter penalty as L2 which basically means the penalty we use in ridge \\n   regression.\\n   \\n   The ridge penalty shrinks the regression coefficient estimate toward zero, but not exactly zero. For this reason,\\n   the ridge regression has long been criticized of not being able to perform variable selection.\\n   '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4\n",
    "'''yes,\n",
    "   We can use ridge regression for feature selection while fitting the model. In this article, we are going to use logistic \n",
    "   regression for model fitting and push the parameter penalty as L2 which basically means the penalty we use in ridge \n",
    "   regression.\n",
    "   \n",
    "   The ridge penalty shrinks the regression coefficient estimate toward zero, but not exactly zero. For this reason,\n",
    "   the ridge regression has long been criticized of not being able to perform variable selection.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa7e381b-b1d0-473c-95bd-dead8b3f3996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When multicollinearity occurs, least squares estimates are unbiased, but their variances are large so they may be far\\n   from the true value. By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors.\\n   \\n   Multicollinearity happens when predictor variables exhibit a correlation among themselves. Ridge regression aims at\\n   reducing the standard error by adding some bias in the estimates of the regression. The reduction of the standard error in \\n   regression estimates significantly increases the reliability of the estimates.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q5\n",
    "'''When multicollinearity occurs, least squares estimates are unbiased, but their variances are large so they may be far\n",
    "   from the true value. By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors.\n",
    "   \n",
    "   Multicollinearity happens when predictor variables exhibit a correlation among themselves. Ridge regression aims at\n",
    "   reducing the standard error by adding some bias in the estimates of the regression. The reduction of the standard error in \n",
    "   regression estimates significantly increases the reliability of the estimates.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a77771c-e044-4878-b5de-559889988208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ridge regression is used for regression purpose only as it needs the dependent variable to be continuous. So for your\\n   analysis Ridge regression can't be used. Special characteristic of Ridge regression is it works fine in presence of\\n   multicollinearity but with a continuous dependent variable.\\n   Categorical variables require special attention in regression analysis because, unlike dichotomous or continuous variables,\\n   they cannot by entered into the regression equation just as they are. Instead, they need to be recoded into a series of \\n   variables which can then be entered into the regression model.\\n   \""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q6\n",
    "'''Ridge regression is used for regression purpose only as it needs the dependent variable to be continuous. So for your\n",
    "   analysis Ridge regression can't be used. Special characteristic of Ridge regression is it works fine in presence of\n",
    "   multicollinearity but with a continuous dependent variable.\n",
    "   Categorical variables require special attention in regression analysis because, unlike dichotomous or continuous variables,\n",
    "   they cannot by entered into the regression equation just as they are. Instead, they need to be recoded into a series of \n",
    "   variables which can then be entered into the regression model.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "474a8b29-d84c-4c57-9a3c-c295930cf473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Interpreting Linear Regression Coefficients\\n     A positive coefficient indicates that as the value of the independent variable increases, the mean of the dependent\\n     variable also tends to increase. A negative coefficient suggests that as the independent variable increases, the \\n     dependent variable tends to decrease.\\n   \\n   Geometric Interpretation of Ridge Regression:\\n   The ridge estimate is given by the point at which the ellipse and the circle touch. There is a trade-off between the\\n   penalty term and RSS. Maybe a large \\x08eta would give you a better residual sum of squares but then it will push the \\n   penalty term hig.\\n   '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q7\n",
    "'''Interpreting Linear Regression Coefficients\n",
    "     A positive coefficient indicates that as the value of the independent variable increases, the mean of the dependent\n",
    "     variable also tends to increase. A negative coefficient suggests that as the independent variable increases, the \n",
    "     dependent variable tends to decrease.\n",
    "   \n",
    "   Geometric Interpretation of Ridge Regression:\n",
    "   The ridge estimate is given by the point at which the ellipse and the circle touch. There is a trade-off between the\n",
    "   penalty term and RSS. Maybe a large \\beta would give you a better residual sum of squares but then it will push the \n",
    "   penalty term hig.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491be748-e2f0-49ce-8ac4-0407d69806f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8\n",
    "'''The ridge regression technique can be used to predict time-series. Ridge regression (RR) can also solve the\n",
    "   multicollinearity problem that exists in linear regression.\n",
    "   Time series regression can help you understand and predict the behavior of dynamic systems from experimental \n",
    "   or observational data. Common uses of time series regression include modeling and forecasting of economic, financial, biological, and engineering systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
